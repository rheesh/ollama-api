import json

import requests

BASE_URL = "http://localhost:11434/api"
MODEL_NAME = "hf.co/unsloth/Qwen3-4B-GGUF:q2_K_L"

HEADERS = {
    "Content-Type": "application/json"
}


class OllamaAPIClient:
    def __init__(self, base_url=BASE_URL, model_name=MODEL_NAME):
        self.base_url = base_url
        self.model_name = model_name
        self.headers = HEADERS

    def list_models(self):
        """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            response = requests.get(f"{self.base_url}/tags", headers=self.headers)
            response.raise_for_status()

            models_data = response.json()
            if "models" in models_data:
                return models_data["models"]
            else:
                print("âŒ ëª¨ë¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

        except requests.exceptions.RequestException as e:
            print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def generate_completion(self, prompt, stream=False, **kwargs):
        """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (completion)"""
        data = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": stream,
            **kwargs
        }
        try:
            if stream:
                return self.proc_stream_response("/generate", data)
            else:
                return self.proc_response("/generate", data, "response")

        except requests.exceptions.RequestException as e:
            print(f"âŒ Generate API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def chat_completion(self, messages, stream=False, **kwargs):
        """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (chat)"""
        data = {
            "model": self.model_name,
            "messages": messages,
            "stream": stream,
            **kwargs
        }

        try:
            if stream:
                return self.proc_stream_response("/chat", data)
            else:
                return self.proc_response("/chat", data, "message")

        except requests.exceptions.RequestException as e:
            print(f"âŒ Chat API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def proc_stream_response(self, endpoint, data):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        full_response = ""

        with requests.post(
                f"{self.base_url}{endpoint}",
                headers=self.headers,
                data=json.dumps(data),
                stream=True
        ) as response:
            response.raise_for_status()

            print("ğŸ”„ ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ:")
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    # completion endpointì˜ ê²½ìš°
                    if "response" in chunk:
                        content = chunk["response"]
                        full_response += content
                        print(content, end="", flush=True)
                    # chat endpointì˜ ê²½ìš°
                    elif "message" in chunk and "content" in chunk["message"]:
                        content = chunk["message"]["content"]
                        full_response += content
                        print(content, end="", flush=True)

        print("\nâœ… ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ!")
        return full_response

    def proc_response(self, endpoint, data, response_key):
        """ì¼ë°˜ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        response = requests.post(
            f"{self.base_url}{endpoint}",
            headers=self.headers,
            data=json.dumps(data)
        )
        response.raise_for_status()

        result = response.json()

        # completion endpointì˜ ê²½ìš°
        if response_key == "response" and "response" in result:
            content = result["response"]
        # chat endpointì˜ ê²½ìš°
        elif response_key == "message" and "message" in result:
            content = result["message"]["content"]
        else:
            print("âŒ ì‘ë‹µì—ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì „ì²´ ì‘ë‹µ: {result}")
            return None

        print("ğŸ¤– ì‘ë‹µ:")
        print(content)
        print("âœ… ì™„ë£Œ!")
        return content



if __name__ == '__main__':

    # Ollama API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = OllamaAPIClient()
    # 6. ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì˜ˆì œ
    print("\n" + "=" * 60)
    client.generate_completion(
        "ë„ˆë¥¼ ì†Œê°œí•´ì¤˜.",
        stream=False,
        options={
            "num_ctx": 30720,
            "temperature": 0.8,  # ì°½ì˜ì„± ì¦ê°€
            "num_predict": -1,  # í† í° ìˆ˜ ì œí•œ, max_tokens
        }
    )
"""
    # 1. ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    models = client.list_models()
    print(models)

    # 2. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± (ì¼ë°˜ ë°©ì‹)
    print("\n" + "=" * 60)
    client.generate_completion(
        "ë„ˆë¥¼ ì†Œê°œí•´ì¤˜.",
        stream=False
    )

    # 3. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
    print("\n" + "=" * 60)
    client.generate_completion(
        "Softwareë¥¼ ê°œë°œí•  ë•Œ, ë„ˆì—ê²Œ ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜?",
        stream=True
    )

    # 4. ëŒ€í™” í˜•ì‹ (ì¼ë°˜ ë°©ì‹)
    print("\n" + "=" * 60)
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì¤˜."}
    ]
    client.chat_completion(messages, stream=False)

    # 5. ëŒ€í™” í˜•ì‹ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
    print("\n" + "=" * 60)
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ Python ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "Pythonì„ ë§Œë“  ì‚¬ëŒì€ ëˆ„êµ¬ì¸ê°€ìš”?"}
    ]
    client.chat_completion(messages, stream=True)
"""
